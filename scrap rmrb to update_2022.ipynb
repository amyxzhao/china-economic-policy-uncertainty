{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urllib.error.HTTPError\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40112/198353825.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m#提取版面链接\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mpagelink\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pageLink'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mres_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"(?<=href=\\\").+?(?=\\\")|(?<=href=\\').+?(?=\\')\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mres_pp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_p\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#the regular expression to find the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''现在的问题是要将这些步骤连接起来:\n",
    "    1. 打开某一天的第一版页面，并爬取当天所有版面的链接，保存在list中\n",
    "    2. 写loop，爬取每个版面的文章列表链接，保存在list中\n",
    "    3. 将列表链接和网站general链接组合起来，保存在list中\n",
    "    4. 写loop，爬取当天报纸每一天的文章\n",
    "    \n",
    "    '''\n",
    "'''第一步：爬取当天所有版面的链接，保存在list中'''\n",
    "#准备好软件包\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import http\n",
    "from random import uniform\n",
    "\n",
    "#提取当天第一版的 URL 的HTML内容\n",
    "\n",
    "\n",
    "a='12'#\n",
    "bb=[]\n",
    "for b0 in range(1,32):#only 30 days in April; print bb to check if it is correct \n",
    "    if b0<10:\n",
    "        b0='0'+str(b0)\n",
    "        bb.append(b0)\n",
    "    else:\n",
    "        b0=str(b0)\n",
    "        bb.append(b0)\n",
    "    \n",
    "    \n",
    "    \n",
    "for b in bb [:32]: \n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n",
    "    general=r'http://paper.people.com.cn/rmrb/html/2024-'+a+'/'+b+'/'  #+a Apil +b month and days\n",
    "    add_g='nbs.D110000renmrb_01.htm'\n",
    "    \n",
    "    newpath = r'C:\\Users\\Yan\\Desktop\\EPU\\RMB\\2024\\2024'+a+b\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    os.chdir(newpath)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        f=urllib.request.urlopen(general+add_g)\n",
    "        response=f.read()\n",
    "    #用BeautifulSoup解析数据，'html.parser'在python3中是必须\n",
    "        html=BeautifulSoup(response,'html.parser')\n",
    "    except urllib.error.HTTPError:\n",
    "        print('urllib.error.HTTPError')  \n",
    "        #出现错误，停几秒先    \n",
    "    except http.client.IncompleteRead:\n",
    "        print('IncompleteRead')\n",
    "        time.sleep(2)\n",
    "        f=urllib.request.urlopen(general+add_g)\n",
    "        response=f.read()\n",
    "    \n",
    "    #提取版面链接\n",
    "    pagelink=html.find_all(id='pageLink')\n",
    "    res_p=r\"(?<=href=\\\").+?(?=\\\")|(?<=href=\\').+?(?=\\')\"\n",
    "    res_pp=re.compile(res_p) #the regular expression to find the URL\n",
    "    links=[]\n",
    "    for i in pagelink:\n",
    "        page=res_pp.findall(str(i))\n",
    "        links.append(page)\n",
    "    \n",
    "    links=[item for sublist in links for item in sublist]\n",
    "    len(links)\n",
    "    \n",
    "    '''2. 写loop，爬取每个版面的文章列表链接，保存在list中'''\n",
    "    \n",
    "    #提取该版面上每篇文章的链接\n",
    "    page_links=[]\n",
    "    for i in links:\n",
    "        page_link=general+i\n",
    "        page_links.append(page_link)\n",
    "        \n",
    "    len(page_links)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #用while，try和exception自动处理错误\n",
    "    title_lists=[]\n",
    "    m=0\n",
    "    while m<len(page_links):\n",
    "        try:\n",
    "            f=urllib.request.urlopen(page_links[m])\n",
    "            response=f.read()\n",
    "            html=BeautifulSoup(response,'html.parser')\n",
    "            title_list= html.find_all('div',attrs={'class':'news'})\n",
    "            titles=res_pp.findall(str(title_list))\n",
    "            title_lists.append(titles)\n",
    "            m=m+1\n",
    "        except http.client.IncompleteRead:\n",
    "            print('IncompleteRead')\n",
    "            time.sleep(2)\n",
    "            m=m\n",
    "        except urllib.error.HTTPError:\n",
    "            print('urllib.error.HTTPError')  \n",
    "            time.sleep(2)#出现错误，停几秒先\n",
    "            m=m+1\n",
    "        except http.client.RemoteDisconnected:\n",
    "            print('RemoteDisconnected')\n",
    "            time.sleep(2)\n",
    "            m=m\n",
    "            \n",
    "    len(title_lists)\n",
    "    \n",
    "    if len(page_links)!=len(title_lists):\n",
    "        print('Alert')\n",
    "    else:\n",
    "        print('Congrats,pass!')\n",
    "        \n",
    "    '''3. 将列表链接和网站general链接组合起来，保存在list中'''\n",
    "    title_links=[]\n",
    "    for i in title_lists:\n",
    "        for j in i:\n",
    "            title_link=general+j\n",
    "            title_links.append(title_link)\n",
    "            \n",
    "    len(title_links)\n",
    "    \n",
    "    '''4. 写loop，爬取当天报纸每一天的文章'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##设置休息时间，以免把server搞摊了\n",
    "    #del title_links[3]\n",
    "    \n",
    "    m=0\n",
    "    while m<len(title_links):\n",
    "        try:\n",
    "            gmrb_request=urllib.request.Request(url=title_links[m], headers=headers)\n",
    "            gmrb_response=urllib.request.urlopen(gmrb_request)\n",
    "            response=gmrb_response.read()\n",
    "            html=BeautifulSoup(response,'html.parser')\n",
    "            title=html.find_all('h1')#找到文章的题目\n",
    "            date_author=html.find_all('p',attrs={'class':'sec'})#找到文章的作者和时间\n",
    "            content=html.find_all(id='articleContent')#找到文章的内容\n",
    "            all_=title+date_author+content\n",
    "            text_all=''\n",
    "            for j in all_:\n",
    "                text=j.get_text()\n",
    "                text_all=text_all+text\n",
    "        #print(text_all)\n",
    "            with open(str(m)+'.txt','w',encoding='utf8') as f:\n",
    "                f.write(text_all)\n",
    "            time.sleep(uniform(2,10))\n",
    "            m=m+1\n",
    "        except http.client.IncompleteRead:\n",
    "            print('IncompleteRead')\n",
    "            time.sleep(uniform(0,2))\n",
    "            m=m\n",
    "        except urllib.error.HTTPError:\n",
    "            print('urllib.error.HTTPError')  \n",
    "            time.sleep(uniform(1,4))#出现错误，停几秒先\n",
    "            m=m+10\n",
    "        except http.client.RemoteDisconnected:\n",
    "            print('RemoteDisconnected')\n",
    "            time.sleep(uniform(1,4))\n",
    "            m=m\n",
    "    \n",
    "    \n",
    "    #import http.client\n",
    "    #http.client.HTTPConnection._http_vsn = 11\n",
    "    #http.client.HTTPConnection._http_vsn_str = 'HTTP/1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
