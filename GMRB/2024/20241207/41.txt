警惕算法偏见和数据遮蔽
作者：韩晓强
《光明日报》（ 2024年12月07日 09版）
  人工智能的应用和算法软件的普及，宣告了法国哲学家保罗·维利里奥所说的“感知的自动化”已然来临。这种自动化意味着将某些工作和决策权让渡给算法和机器，与此同时，人类可以将更多时间用于娱乐和休闲，文艺创作将会更加便捷，诸如“人机协同”“人机共创”“技术赋能”等说法层出不穷，但算法和机器又会带来什么样的风险？人类真的可以将更多的权能交托给看似“无利害”的算法吗？
基于标签/分类的算法偏见
  尽管算法作为一个非人类的决策者显得客观中立，但我们需要考察的正是这种“中立性”是否名副其实。这里不妨将目光投向当代最常见的算法——图像识别技术，在种种图像识别系统中，我们不难见到算法会习惯性地为图像中的人物自动打上某些标签：农民、孤儿、流浪汉等。但从一个人类观察者看来，这类识别定性的标签既不符合实情，也没有确凿的依据。
  国外的一些图像识别系统还存在对有色人种的潜在偏见，甚至出现过将黑人识别为“大猩猩”的情况。并且有的聊天机器人会使用某些种族、性别歧视的语言，社交媒体向男性推送的高薪招聘信息往往多于女性，同时对女性的求职简历进行权重降级。这些基于种族、性别等的默认分类，正是依赖于现实世界中默认的自然分类，也即文化中的惯有偏见，它们只不过以一种隐性的、无意识的方式在算法中映射出来。
  即便一个现实中的人可能不承认自己有某些偏见，但在一些默认表达的数据生产中，他也不免会沿袭相应的分类。目前算法的数据集和训练集都依赖于收集海量的数据，但这些数据本身就呈现出某种程度的偏见。同时，算法的执行并不基于语义层面，它仅仅处理符号，也就是图像相应的标签——每个图像都会被打上一系列固定的标签，它们并非由机器预先设定，而是由廉价乃至免费的众包劳动力（平台将各种形式的数字劳动外包给数字工作者，并通过自动化评价、管理劳动力）来完成。事实上，每次我们进入一些网站，被要求进行“人类核验”（即证明登录者是真实的人类）时，我们点击图片中红绿灯、斑马线、公交车、自行车等图像的行为都是在训练分类识别算法。
  在类似的众包劳动中，最典型的莫过于亚马逊公司的人工数字服务。在这个平台之上，诸多远程工作者用自己零散且报酬极低的工作来为图像进行分类和标签化，并在这个过程中训练和改进诸多高科技公司的算法系统。作为一个大型数据库，图网（亚马逊公司人工数字服务的雇主）将超过1400万张图片进行了手动注释，这些人工标签化的活动必然会潜在地引入注释者本人的某些偏见，即便是深度无意识的偏见。被众包工人训练的算法则会顺利继承这种偏见，由此产生了前文当中提及的“算法偏见”。
  美国当代学者凯特·克劳福德与国际知名艺术家特雷弗·帕格伦在2018年启动了《图网轮盘》研究项目，他们询问的是：这些图片从哪里来？照片中的人为什么会被贴上这样的标签？当图片与标签配对时，有什么样的因素在起作用？当它们被用来训练技术系统时，又有什么样的影响？这一研究显现了人工智能算法系统中确实复刻乃至强化了来自社会现实的固有偏见。如果忽视它，这些偏见将会在飞速发展的技术中加速沉淀、渗入未来。
基于概率/卷积的数据遮蔽
  在算法偏见之外，一个同样重要的隐患在于算法造成的“数据遮蔽”，尽管算法操作会调用大量的数据，对其进行提取、整合、分析，但这些操作仍然基于概率，即针对数据在整个数据集中出现的次数和频率，这意味着它优先抓取那些出现频次最高的“优势数据”，而忽略那些几乎无人问津的“少数数据”或“稀有数据”。
  我们当然不能以出现的频次来判断一个数据的价值，因为它仅仅是一种注意力或流量意义上的价值，不代表实际价值。现实中最重要的知识和信息，有时恰恰来源于这些少数或稀有的数据，但在如今的算法推送、数据挖掘和信息检索系统中，它们越来越难以出现在数据流的表面。国外媒介理论家列夫·马诺维奇曾经举过一个例子：一个乡下博物馆有一幅著名画家的稀有真迹，如果这个博物馆没有观众入内，或者零星的观众没有对其进行拍照和上传至社交媒体，那么算法就无法捕捉到它的信息。然而一旦有人发现了这幅作品，并在社交媒体上传播、发酵，引来了更多游客拍照打卡，那么这件艺术品就成为一个显性的数据。
  我们很难想象任何一个孤立的作品会有这样的好运，在绝大多数情况下，它们只能位于数据底层的深处和边角，处在数据挖掘探测不到、数据提取无法触及之所。更重要的是，随着数据生产的体量激增，数据生产的速率加快，新创造的数据会呈现出更大程度的“数据堆积”，这些堆积的数据仍然依赖于概率生产，即对那些数据集中出现频次最多、最热门、最多搜索痕迹的数据进行再加工，这一方面会让当下的数据产生巨大的同质性，同时也会将那些“少数数据”推至越来越深的数据底层。如此，同质化的数据呈现出一种自我迭代和卷积，而那些少数的数据既不会产生迭代，也不会被挤压在一起，它们只能以越来越分散、越来越稀薄的方式散落到数据库的边角。
  在这种情况下，即便再强的算力也无法挖掘到这些数据，无法触及这些角落。因为算力只是服务于算法的规则，来处理越来越海量的优势数据，“少数数据”只能日渐下沉，直到完全脱离算法的搜索范围。一旦步入这种境地，我们就可以说“这些数据不存在”，因为算法再也无法捕获它们；但在真实的数据库中，这些数据又确实存在。按照一种现实性的按图索骥，按照一种线性的索引关系，我们始终能够以传统的方式从某个图书馆或档案馆的书目、资料、信笺中找到需要的数据。这种古老的方法显得笨拙、单调且需要花费太多气力，但在寻找和提取少数数据的过程中，它仍然是比算法检索更为可靠的方式。
  因此，越大的数据体量、越自动化的算法模式，就可能会带来更大程度的数据遮蔽。数据遮蔽既会导致知识和机遇的流失，也会造成文化单一的现实问题，正如美国科学家乔恩·克莱因伯格所说：“如果我们都使用同一种算法做决定，是否会导致作出的决定高度趋同，导致我们的文化也是高度趋同？”
尽力确保算法在各个可及层面上公平
  算法偏见与数据遮蔽，最终指向了一个典型问题，即在基于自动化的感知系统中，算法与生俱来就带有偏见，而大数据则自然地倾向于遮蔽和自我卷积。尽管这些问题给一些算法企业带来直接的伦理压力，让他们不得不改革既有的技术，调整算法的模式，让其看上去更为合理。但诚如克劳福德所说，这些企业更倾向于从表面上解决这些明显的技术错误。这种临时方案仅仅是建立数学意义上的平等以产生“更公平的系统”，但并不致力于改变潜在的不合理结构。
  所以，问题不止在技术修复，而是要认真审视算法的整个数据挖掘、提取、分类和分析测算的流程，并在整个流程中思考“公平”的问题。算法公平并非单一的标准，而是多样的标准，应确保它在各个可及的层面上都是公平的。这就需要对公平性指标进行新的评估，让量化指标凸显不同群体之间的相关差异。
  在我看来，算法模型应基于如下的三种原则：
  数据公平原则：确保可探测、可挖掘的数据集中包含尽可能多类型的数据，包括那些出现频次极低的少数或稀有数据。这不但需要通过数据增广，还需要重新评估数据的权重，对那些少数或稀有的数据进行加权处理，以抵消优势数据不断卷积造成的数据遮蔽，并且让算法挖掘到更深的边角或底层。
  模型公平原则：算法模型必须考虑到不同群体，尤其是那些少数群体的利益，建立一种基于公平性约束的迭代系统（能及时纠错），即让算法学习并提升自我的公平感知梯度。
  监督公平原则：无论什么样类型和体量的平台，都应以适当方式公布算法推荐服务的基本原理、目的、意图、主要运行机制，确保简单、清晰、可理解，接受公众的监督。
  尽管上述问题是缓解算法偏见与数据遮蔽的可行方案，但克服这一切问题的根源仍在于人类文明的进程。算法问题是一个社会问题，而非单纯的科学问题，这需要全人类社会的长期共同努力。
  （作者：韩晓强，系西南政法大学新闻传播学院副教授）

